# MedExtract Configuration

# Ollama Configuration
OLLAMA_HOST=http://ollama:11434
OLLAMA_NUM_PARALLEL=2
OLLAMA_MAX_LOADED_MODELS=2

# Backend Configuration
BACKEND_PORT=8000
BACKEND_HOST=0.0.0.0
LOG_LEVEL=INFO

# Frontend Configuration
FRONTEND_PORT=3000
NEXT_PUBLIC_API_URL=http://localhost:8000

# Model Selection (for CPU vs GPU)
# For GPU: phi4:latest, llama3.2:3b, mixtral:8x7b
# For CPU: phi3:mini, tinyllama:latest, orca-mini:3b
DEFAULT_MODEL=phi3:mini

# RAG Configuration
USE_RAG_DEFAULT=true
CHUNK_SIZE=800
CHUNK_OVERLAP=150

# Processing Configuration
BATCH_SIZE=5
CHECKPOINT_FREQUENCY=10
MAX_WORKERS=4

# Data Paths
DATA_PATH=./data
OUTPUT_PATH=./output
CHECKPOINT_PATH=./checkpoints